{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn import cross_validation, grid_search, metrics, neighbors, tree, naive_bayes, ensemble, svm, calibration, feature_selection, pipeline ,preprocessing\n",
    "\n",
    "pd.set_option(\"display.max_columns\",60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Social honeypot icwsm 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Спамеры\n",
    "content_polluters = pd.read_table('./content_polluters.txt', header=None, names = [\n",
    "    \"UserID\",\n",
    "    \"ProfileCreatedAt\",\n",
    "    \"ProfileCollectedAt\",\n",
    "    \"NumberOfFollowings\",\n",
    "    \"NumberOfFollowers\",\n",
    "    \"NumberOfTweets\",\n",
    "    \"LengthOfScreenName\",\n",
    "    \"LengthOfDesc\"])\n",
    "content_polluters_tweets = pd.read_table('./content_polluters_tweets.txt', header=None, names = [\n",
    "    \"UserID\",\n",
    "    \"TweetID\",\n",
    "    \"Tweet\",\n",
    "    \"TweetCreatedAt\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Пользователь 1:1 Твит (последний по дате публикации)\n",
    "content_polluters = content_polluters.set_index(\"UserID\")\n",
    "content_polluters_tweets_distinct = content_polluters_tweets.sort_values('TweetCreatedAt', ascending = False).groupby(['UserID']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Легитимные пользователи\n",
    "legitimate_users = pd.read_table('./legitimate_users.txt', header=None, names = [\n",
    "    \"UserID\",\n",
    "    \"ProfileCreatedAt\",\n",
    "    \"ProfileCollectedAt\",\n",
    "    \"NumberOfFollowings\",\n",
    "    \"NumberOfFollowers\",\n",
    "    \"NumberOfTweets\",\n",
    "    \"LengthOfScreenName\",\n",
    "    \"LengthOfDesc\"])\n",
    "legitimate_users_tweets = pd.read_table('./legitimate_users_tweets.txt', header=None, names = [\n",
    "    \"UserID\",\n",
    "    \"TweetID\",\n",
    "    \"Tweet\",\n",
    "    \"TweetCreatedAt\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Пользователь 1:1 Твит (последний по дате публикации)\n",
    "legitimate_users = legitimate_users.set_index(\"UserID\")\n",
    "legitimate_users_tweets_distinct = legitimate_users_tweets.sort_values('TweetCreatedAt', ascending = False).groupby(['UserID']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of spamwords\n",
    "with open(\"blacklist.txt\", \"r\", encoding='utf-8') as f:\n",
    "    spamwords=f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spammers = pd.concat([content_polluters, content_polluters_tweets_distinct], axis=1, join='inner')\n",
    "spammers['IsSpammer'] = 1\n",
    "\n",
    "nonspammers =  pd.concat([legitimate_users, legitimate_users_tweets_distinct], axis=1, join='inner')\n",
    "nonspammers['IsSpammer'] = 0\n",
    "\n",
    "data = pd.concat([spammers, nonspammers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#User features\n",
    "data[\"ProfileCreatedAt\"] = pd.to_datetime(data[\"ProfileCreatedAt\"])\n",
    "data[\"ProfileCollectedAt\"] = pd.to_datetime(data[\"ProfileCollectedAt\"])\n",
    "data[\"AccountAge\"] = (data[\"ProfileCollectedAt\"] - data[\"ProfileCreatedAt\"]).astype('timedelta64[h]')\n",
    "\n",
    "data[\"Flwrs/Flwngs\"] = data[\"NumberOfFollowers\"].divide(data[\"NumberOfFollowings\"], fill_value=True)\n",
    "data[\"Flwrs/Flwngs\"].replace(np.inf, data[\"NumberOfFollowers\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"Reputation\"] = data[\"NumberOfFollowers\"].divide((data[\"NumberOfFollowers\"] + data[\"NumberOfFollowings\"]), fill_value=True)\n",
    "data[\"Reputation\"].replace(np.inf, data[\"NumberOfFollowers\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"FlwngsIncrease\"] = data[\"NumberOfFollowings\"].divide(data[\"AccountAge\"])\n",
    "data[\"FlwngsIncrease\"].replace(np.inf, data[\"NumberOfFollowings\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"TweetsPerDay\"]  = data[\"NumberOfTweets\"].divide((data[\"AccountAge\"] / 24), fill_value=True)\n",
    "data[\"TweetsPerDay\"].replace(np.inf, data[\"NumberOfTweets\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"TweetsPerWeek\"]  = data[\"NumberOfTweets\"].divide((data[\"AccountAge\"] / (24 * 7)), fill_value=True)\n",
    "data[\"TweetsPerWeek\"].replace(np.inf, data[\"NumberOfTweets\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tweet(row):\n",
    "    tweet = row[\"TweetWordsList\"]\n",
    "    cl_tweet = []\n",
    "    for word in tweet:\n",
    "        m = re.match(\"https://\\S+|http://\\S+|bit.ly\\S+\", word)\n",
    "        if word[0] not in ['@', '#'] and not m:\n",
    "            cl_tweet.append(word)\n",
    "    return ' '.join(cl_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_urls(row):\n",
    "    tweet = row[\"TweetWordsList\"]\n",
    "    urls = 0\n",
    "    for word in tweet:\n",
    "        m = re.match(\"https://\\S+|http://\\S+|bit.ly\\S+\", word)\n",
    "        if m:\n",
    "            urls += 1\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_hashtags(row):\n",
    "    tweet = row[\"TweetWordsList\"]\n",
    "    hashtags = 0\n",
    "    for word in tweet:\n",
    "        if word[0] == '#':\n",
    "            hashtags += 1\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_mentions(row):\n",
    "    tweet = row[\"TweetWordsList\"]\n",
    "    mentions = 0\n",
    "    for word in tweet:\n",
    "        if word[0] == '@':\n",
    "            mentions += 1\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_uppercases(row):\n",
    "    tweet = row[\"CleanTweetWordsList\"]\n",
    "    uppercases = 0\n",
    "    for word in tweet:\n",
    "        if word[0].isupper():\n",
    "            uppercases += 1\n",
    "    return uppercases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def longest_word(row):\n",
    "    tweet = row[\"CleanTweetWordsList\"]\n",
    "    maxword = 0\n",
    "    for word in tweet:\n",
    "        if len(word) > maxword:\n",
    "            maxword = len(word)\n",
    "    return maxword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_word(row):\n",
    "    tweet = row[\"CleanTweetWordsList\"]\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for word in tweet:\n",
    "        sum += len(word)\n",
    "        count += 1\n",
    "    try:\n",
    "        avg = sum/count\n",
    "        return avg\n",
    "    except ZeroDivisionError as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_spamwords(row):\n",
    "    tweet = row[\"CleanTweetWordsList\"]\n",
    "    spam = 0\n",
    "    for word in tweet:\n",
    "        if word in spamwords:\n",
    "            spam += 1\n",
    "    return spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"TweetWordsList\"] = data[\"Tweet\"].str.split()\n",
    "data['CleanTweet'] = data.apply(clean_tweet,axis=1)\n",
    "data[\"CleanTweetWordsList\"] = data[\"CleanTweet\"].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Content features\n",
    "data[\"NumberOfWords\"] = data[\"CleanTweet\"].str.count(' ') + 1\n",
    "data[\"NumberOfWords\"].replace(1, 0, inplace = True)\n",
    "\n",
    "data[\"NumberOfSymbols\"] = data[\"Tweet\"].str.len()\n",
    "\n",
    "data[\"NumberOfSpaces\"] = data[\"Tweet\"].str.count(' ')\n",
    "\n",
    "data[\"NumberOfCl\"] = data.apply(count_uppercases,axis=1)\n",
    "\n",
    "data[\"ClperWord\"] = data[\"NumberOfCl\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"ClperWord\"].replace(np.inf, data[\"NumberOfCl\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data['MaxWord'] = data.apply(longest_word,axis=1)\n",
    "\n",
    "data['AvgWord'] = data.apply(average_word,axis=1)\n",
    "\n",
    "data[\"NumberOfEm\"] = data[\"Tweet\"].str.count('\\!')\n",
    "\n",
    "data[\"NumberOfQm\"] = data[\"Tweet\"].str.count('\\?')\n",
    "\n",
    "data[\"NumberOfURLs\"] = data.apply(count_urls,axis=1)\n",
    "\n",
    "data[\"URLperWord\"] = data[\"NumberOfURLs\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"URLperWord\"].replace(np.inf, data[\"NumberOfURLs\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "\n",
    "data[\"NumberOfMentions\"] = data.apply(count_mentions,axis=1)\n",
    "\n",
    "data[\"MentionsperWord\"] = data[\"NumberOfMentions\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"MentionsperWord\"].replace(np.inf, data[\"NumberOfMentions\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"NumberOfHashtags\"] = data.apply(count_hashtags,axis=1)\n",
    "\n",
    "data[\"HashtagsperWord\"] = data[\"NumberOfHashtags\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"HashtagsperWord\"].replace(np.inf, data[\"NumberOfHashtags\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"NumberOfSpamWords\"] = data.apply(count_spamwords,axis=1)\n",
    "\n",
    "data[\"SpamWordsperWord\"] = data[\"NumberOfSpamWords\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"SpamWordsperWord\"].replace(np.inf, data[\"NumberOfSpamWords\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop Tweets longer than 140 chars \n",
    "data = data.drop(data[data.NumberOfSymbols > 140].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,7)\n",
    "plt.rcParams['font.size'] = 25   \n",
    "\n",
    "data0=data[data.Reputation > .1]\n",
    "plt.hist([data0[data0.IsSpammer==1].Reputation.values,\n",
    "          data0[data0.IsSpammer==0].Reputation.values],\n",
    "          label = ['Спамеры', 'Легитимные аккаунты'],\n",
    "          alpha = .99)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Репутация\")\n",
    "plt.ylabel(\"Количество пользователей\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_cols = { \"UserFeatures\" : [\n",
    "                                \"LengthOfScreenName\",\n",
    "                                \"LengthOfDesc\",\n",
    "                                \"NumberOfFollowings\",\n",
    "                                \"NumberOfFollowers\",\n",
    "                                \"NumberOfTweets\",\n",
    "                                \"AccountAge\",\n",
    "                                \"Flwrs/Flwngs\",\n",
    "                                \"Reputation\",\n",
    "                                \"FlwngsIncrease\",\n",
    "                                \"TweetsPerDay\",\n",
    "                                \"TweetsPerWeek\"\n",
    "                                ],\n",
    "               \"ContentFeatures\":[ \n",
    "                                  \"NumberOfWords\",\n",
    "                                  \"NumberOfSymbols\",\n",
    "                                  \"NumberOfSpaces\",\n",
    "                                  \"NumberOfCl\",\n",
    "                                  \"ClperWord\",\n",
    "                                  \"MaxWord\",\n",
    "                                  \"AvgWord\",\n",
    "                                  \"NumberOfEm\",\n",
    "                                  \"NumberOfQm\",\n",
    "                                  \"NumberOfURLs\",\n",
    "                                  \"URLperWord\",\n",
    "                                  \"NumberOfHashtags\",\n",
    "                                  \"HashtagsperWord\",\n",
    "                                  \"NumberOfMentions\",\n",
    "                                  \"MentionsperWord\",\n",
    "                                  \"NumberOfSpamWords\",\n",
    "                                  \"SpamWordsperWord\"\n",
    "                                ],\n",
    " \n",
    "               }\n",
    "X = data[feature_cols[\"UserFeatures\"] + feature_cols[\"ContentFeatures\"]]\n",
    "Y = data['IsSpammer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "X = (X - X.mean())/ ((X.max() - X.min())/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, train_size=0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_report(RealValues, PredictedValues ):\n",
    "    print('Accuracy: ' + str(metrics.accuracy_score(RealValues, PredictedValues)))\n",
    "    print('\\n' + 'Classification report:')\n",
    "    print(metrics.classification_report(RealValues, PredictedValues))\n",
    "    print('\\n' + 'Confusion matrix: ')\n",
    "    print(metrics.confusion_matrix(RealValues, PredictedValues))\n",
    "\n",
    "def print_cross_validation(model, X, Y):\n",
    "    scores = cross_validation.cross_val_score(model, X, Y, cv=10, scoring='accuracy')\n",
    "    print(scores)\n",
    "    print('Cross Validation Score: ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = cross_validation.StratifiedShuffleSplit(Y_train, n_iter = 10, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bern = naive_bayes.BernoulliNB()\n",
    "bern.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'fit_prior' : [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = grid_search.GridSearchCV(bern, parameters_grid, scoring = 'accuracy', cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimator = grid_cv.best_estimator_\n",
    "Y_pred = best_estimator.predict(X_test)\n",
    "print_report(Y_test, Y_pred)\n",
    "print_cross_validation(best_estimator, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (grid_cv.best_score_)\n",
    "print (grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'n_neighbors' : [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n",
    "    'weights' :  ['uniform', 'distance'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_cv = grid_search.GridSearchCV(knn, parameters_grid, scoring = 'accuracy', cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimator = grid_cv.best_estimator_\n",
    "Y_pred = best_estimator.predict(X_test)\n",
    "print_report(Y_test, Y_pred)\n",
    "print_cross_validation(best_estimator, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (grid_cv.best_score_)\n",
    "print (grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = svm.svc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " parameters_grid = {\n",
    "     'kernel' : ['linear', 'poly', 'rbf', 'sigmoid' ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = grid_search.GridSearchCV(svc, parameters_grid, scoring = 'accuracy', cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_estimator = grid_cv.best_estimator_\n",
    "Y_pred = best_estimator.predict(X_test)\n",
    "print_report(Y_test, Y_pred)\n",
    "print_cross_validation(best_estimator, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (grid_cv.best_score_)\n",
    "print (grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'criterion'  : ['gini', 'entropy'],\n",
    "    'splitter' : ['best','random'],\n",
    "    'max_depth' : [1, 2, 3, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = grid_search.GridSearchCV(dt, parameters_grid, scoring = 'accuracy', cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimator = grid_cv.best_estimator_\n",
    "Y_pred = best_estimator.predict(X_test)\n",
    "print_report(Y_test, Y_pred)\n",
    "print_cross_validation(best_estimator, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (grid_cv.best_score_)\n",
    "print (grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "#rfc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'n_estimators' : list(range(1,20)),\n",
    "    'criterion' : ['gini','entropy'],\n",
    "    'max_depth' : [1,2,3,4,5,None],\n",
    "    'bootstrap' : [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = grid_search.GridSearchCV(rfc, parameters_grid, scoring = 'accuracy', cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_estimator = grid_cv.best_estimator_\n",
    "Y_pred = best_estimator.predict(X_test)\n",
    "print_report(Y_test, Y_pred)\n",
    "print_cross_validation(best_estimator, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (grid_cv.best_score_)\n",
    "print (grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_cv.grid_scores_[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances  = best_estimator.feature_importances_\n",
    "std = np.std([best_estimator.feature_importances_ for tree in best_estimator.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20,7)\n",
    "plt.rcParams['font.size'] = 15 \n",
    "plt.figure()\n",
    "plt.title(\"Полезность признаков\")\n",
    "plt.bar(range(X_test.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_test.shape[1]), list(X_test[indices]), rotation='vertical')\n",
    "plt.xlim([-1, X_test.shape[1]])\n",
    "plt.gcf().subplots_adjust(bottom=0.5)\n",
    "#plt.setp(legend.get_title(),fontsize='xx-small')\n",
    "plt.savefig(\"featureImportance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,7)\n",
    "plt.rcParams['font.size'] = 25   \n",
    "\n",
    "data0=data\n",
    "plt.hist([data0[data0.IsSpammer==1].FlwngsIncrease.values,\n",
    "          data0[data0.IsSpammer==0].FlwngsIncrease.values],\n",
    "          label = ['Спамеры','Легитимные пользователи'],\n",
    "          alpha = .99,\n",
    "          bins=range(10,100, 5))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Прирост подписок (ПС/ВА)\")\n",
    "plt.ylabel(\"Количество пользователей\")\n",
    "plt.savefig(\"flwngsincrease.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,7)\n",
    "plt.rcParams['font.size'] = 25   \n",
    "\n",
    "data0=data\n",
    "plt.hist([data0[data0.IsSpammer==1].NumberOfFollowings.values,\n",
    "          data0[data0.IsSpammer==0].NumberOfFollowings.values],\n",
    "          label = ['Спамеры','Легитимные пользователи'],\n",
    "          alpha = .99,\n",
    "          bins=range(10,200, 10))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Количество подписок\")\n",
    "plt.ylabel(\"Количество пользователей\")\n",
    "plt.savefig(\"NumberOfFollowings.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,7)\n",
    "plt.rcParams['font.size'] = 25   \n",
    "\n",
    "data0=data\n",
    "plt.hist([data0[data0.IsSpammer==1].NumberOfTweets.values,\n",
    "          data0[data0.IsSpammer==0].NumberOfTweets.values],\n",
    "          label = ['Спамеры','Легитимные пользователи'],\n",
    "          alpha = .99,\n",
    "          bins=range(10,1010, 10))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Количество твитов\")\n",
    "plt.ylabel(\"Количество пользователей\")\n",
    "#plt.savefig(\"reputation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result(row):\n",
    "    if not row['IsCorrect'] and row['Actual'] == 0:\n",
    "        return \"FalsePositive\"\n",
    "    elif not row['IsCorrect'] and row['Actual'] == 1:\n",
    "        return \"FalseNegative\"\n",
    "    elif row['IsCorrect'] and row['Actual'] == 1:\n",
    "        return \"TruePositive\"\n",
    "    elif row['IsCorrect'] and row['Actual'] == 0:\n",
    "        return \"TrueNegative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Compare = {\"Predicted\" : Y_pred, 'Actual': Y_test, 'IsCorrect': Y_pred == Y_test}\n",
    "Tests = pd.DataFrame(data=Compare)\n",
    "Tests['Result'] = Tests.apply(result,axis=1) \n",
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tests.loc[Tests['Result'] == 'FalsePositive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FalsePositives = data.loc[Tests.loc[Tests['Result'] == 'FalsePositive'].index]  # .to_latex()\n",
    "URLS = 100. * FalsePositives.NumberOfURLs.value_counts() / len(FalsePositives.NumberOfURLs)\n",
    "URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TweetExamples = FalsePositives.loc[[83076908,58205927,27059567,27094415,674433]]\n",
    "TweetExamples.Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x = data['AccountAge'].loc[data['IsSpammer'] == 1], y=data['NumberOfFollowings'].loc[data['IsSpammer'] == 1],   color = 'Red', label = \"Спамовые аккаунты\")\n",
    "ax1.scatter(x = data['AccountAge'].loc[data['IsSpammer'] == 0], y=data['NumberOfFollowings'].loc[data['IsSpammer'] == 0],  color = 'Green', label= \"Легитимные аккаунты\")\n",
    "ax1.scatter(x = data['AccountAge'].loc[Tests.loc[Tests['Result'] == 'FalseNegative'].index], y=data['NumberOfFollowings'].loc[Tests.loc[Tests['Result'] == 'FalseNegative'].index],  color = 'Yellow', label= \"False Negatives\")\n",
    "ax1.scatter(x = data['AccountAge'].loc[Tests.loc[Tests['Result'] == 'FalsePositive'].index], y=data['NumberOfFollowings'].loc[Tests.loc[Tests['Result'] == 'FalsePositive'].index], color = 'Black', label= \"False Positives\")\n",
    "plt.xlabel(\"Возраст аккаунта\")\n",
    "plt.ylabel(\"Количествоподписок\")\n",
    "plt.legend(loc='upper right');\n",
    "plt.savefig(\"ageincreasewitherrors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.concat([spammers, nonspammers])\n",
    "data = data[:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#User features\n",
    "\n",
    "# LengthOfScreenName\n",
    "# LengthOfDescriptionInUserProfile\n",
    "# NumberofFollowings\n",
    "# NumberofFollowers\n",
    "# NumberofTweets\n",
    "data[\"ProfileCreatedAt\"] = pd.to_datetime(data[\"ProfileCreatedAt\"])\n",
    "data[\"ProfileCollectedAt\"] = pd.to_datetime(data[\"ProfileCollectedAt\"])\n",
    "data[\"AccountAge\"] = (data[\"ProfileCollectedAt\"] - data[\"ProfileCreatedAt\"]).astype('timedelta64[h]')\n",
    "\n",
    "data[\"Flwrs/Flwngs\"] = data[\"NumberOfFollowers\"].divide(data[\"NumberOfFollowings\"], fill_value=True)\n",
    "data[\"Flwrs/Flwngs\"].replace(np.inf, data[\"NumberOfFollowers\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"Reputation\"] = data[\"NumberOfFollowers\"].divide((data[\"NumberOfFollowers\"] + data[\"NumberOfFollowings\"]), fill_value=True)\n",
    "data[\"Reputation\"].replace(np.inf, data[\"NumberOfFollowers\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"FlwngsIncrease\"] = data[\"NumberOfFollowings\"].divide(data[\"AccountAge\"])\n",
    "data[\"FlwngsIncrease\"].replace(np.inf, data[\"NumberOfFollowings\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"TweetsPerDay\"]  = data[\"NumberOfTweets\"].divide((data[\"AccountAge\"] / 24), fill_value=True)\n",
    "data[\"TweetsPerDay\"].replace(np.inf, data[\"NumberOfTweets\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"TweetsPerWeek\"]  = data[\"NumberOfTweets\"].divide((data[\"AccountAge\"] / (24 * 7)), fill_value=True)\n",
    "data[\"TweetsPerWeek\"].replace(np.inf, data[\"NumberOfTweets\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"TweetWordsList\"] = data[\"Tweet\"].str.split()\n",
    "data['CleanTweet'] = data.apply(clean_tweet,axis=1)\n",
    "data[\"CleanTweetWordsList\"] = data[\"CleanTweet\"].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Content features\n",
    "\n",
    "data[\"NumberOfWords\"] = data[\"CleanTweet\"].str.count(' ') + 1\n",
    "data[\"NumberOfWords\"].replace(1, 0, inplace = True)\n",
    "\n",
    "data[\"NumberOfSymbols\"] = data[\"Tweet\"].str.len()\n",
    "\n",
    "data[\"NumberOfSpaces\"] = data[\"Tweet\"].str.count(' ')\n",
    "\n",
    "data[\"NumberOfCl\"] = data.apply(count_uppercases,axis=1)\n",
    "\n",
    "data[\"ClperWord\"] = data[\"NumberOfCl\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"ClperWord\"].replace(np.inf, data[\"NumberOfCl\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data['MaxWord'] = data.apply(longest_word,axis=1)\n",
    "\n",
    "data['AvgWord'] = data.apply(average_word,axis=1)\n",
    "\n",
    "data[\"NumberOfEm\"] = data[\"Tweet\"].str.count('\\!')\n",
    "\n",
    "data[\"NumberOfQm\"] = data[\"Tweet\"].str.count('\\?')\n",
    "\n",
    "data[\"NumberOfURLs\"] = data.apply(count_urls,axis=1)\n",
    "\n",
    "data[\"URLperWord\"] = data[\"NumberOfURLs\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"URLperWord\"].replace(np.inf, data[\"NumberOfURLs\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "\n",
    "data[\"NumberOfMentions\"] = data.apply(count_mentions,axis=1)\n",
    "\n",
    "data[\"MentionsperWord\"] = data[\"NumberOfMentions\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"MentionsperWord\"].replace(np.inf, data[\"NumberOfMentions\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"NumberOfHashtags\"] = data.apply(count_hashtags,axis=1)\n",
    "\n",
    "data[\"HashtagsperWord\"] = data[\"NumberOfHashtags\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"HashtagsperWord\"].replace(np.inf, data[\"NumberOfHashtags\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "data[\"NumberOfSpamWords\"] = data.apply(count_spamwords,axis=1)\n",
    "\n",
    "data[\"SpamWordsperWord\"] = data[\"NumberOfSpamWords\"].divide(data[\"NumberOfWords\"])\n",
    "data[\"SpamWordsperWord\"].replace(np.inf, data[\"NumberOfSpamWords\"], inplace = True)\n",
    "data.replace(np.nan, 0, inplace = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
